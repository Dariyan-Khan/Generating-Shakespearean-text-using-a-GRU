{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the shakespeare text dataset and open the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the different elements in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a tokenizer at the character level, so each character is associated a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a phrase being turned to a vector and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 2, 12, 12, 4, 1, 17, 4, 9, 12, 13]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"hello world\"])\n",
    "#Note that the 1 in the vector is the space between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h e l l o   w o r l d']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[7, 2, 12, 12, 4, 1, 17, 4, 9, 12, 13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1115394)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "(max_id,dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text and create a training section of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into windows of length 101, where in window of 101 characters, we use the first 100 characters to try and predict the 101st character. Our first window will be the 1st-101st letter. The second window will bbe from the 2nd letter to the 102nd letter etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds to keep results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch the dataset into sets of 32 windows and one-hot encode the different characters in the X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a GRU based model and use adam as our optimizer. This problem also lends itself to the sparse_categorical_cross_entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31370/31370 [==============================] - 11946s 381ms/step - loss: 1.4661\n",
      "Epoch 2/10\n",
      "31370/31370 [==============================] - 8210s 262ms/step - loss: 1.3634\n",
      "Epoch 3/10\n",
      "31370/31370 [==============================] - 7968s 254ms/step - loss: 1.3437\n",
      "Epoch 4/10\n",
      "31370/31370 [==============================] - 8707s 278ms/step - loss: 1.3326\n",
      "Epoch 5/10\n",
      "31370/31370 [==============================] - 9842s 314ms/step - loss: 1.3253\n",
      "Epoch 6/10\n",
      "31370/31370 [==============================] - 11645s 371ms/step - loss: 1.3200\n",
      "Epoch 7/10\n",
      "31370/31370 [==============================] - 10854s 346ms/step - loss: 1.3157\n",
      "Epoch 8/10\n",
      "31370/31370 [==============================] - 9241s 295ms/step - loss: 1.3128\n",
      "Epoch 9/10\n",
      "31370/31370 [==============================] - 8246s 263ms/step - loss: 1.3104\n",
      "Epoch 10/10\n",
      "31370/31370 [==============================] - 11798s 376ms/step - loss: 1.3084\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to allow us to make predictions more easily. The number of characters to predict is set to 1000 in the complete_text function. The first letter needs to be specified when using the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X=np.array(tokenizer.texts_to_sequences(texts))-1\n",
    "    return tf.one_hot(X,max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new=preprocess([text])\n",
    "    y_proba=model.predict(X_new)[0,-1:,:]\n",
    "    rescaled_logits=tf.math.log(y_proba)/temperature\n",
    "    char_id=tf.random.categorical(rescaled_logits, num_samples=1)+1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text,n_chars=1000,temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text+=next_char(text,temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of generating random text, and randomly generating an initial letter to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong, the city.\n",
      "\n",
      "hortensio:\n",
      "give himself, receive the rightal blow the rest,\n",
      "they burn users, that must tell this give be honour\n",
      "and me all particially as for it;\n",
      "which gives a suitor heard withal instrued\n",
      "and make god pirped in althe takes oc very deeds.\n",
      "\n",
      "painio:\n",
      "alive to common; even are away, i may leave her\n",
      "balls pass, since i proceed and live it find for\n",
      "the stolence hath thus flound the state,\n",
      "but knock you faol, was more a counsel for the good the belly\n",
      "would be loud and so.\n",
      "behive you what you will not hake? you danciy perhaps of me;\n",
      "he'll part the worsporants ere it?\n",
      "\n",
      "maniantio:\n",
      "become it that shall socient the good master,\n",
      "this good fyceinor, well\n",
      "his faults, i have her colding me not in my tears,\n",
      "and do i will content to be marcived by a fair in hell:\n",
      "if i proceed us belly once.\n",
      "\n",
      "bianca:\n",
      "who shall be countent to her way to leave not\n",
      "his own prince well; even be not rates, i did no foll\n",
      "if you plead boar beneyous the ready us words:\n",
      "why ched you state, the opentastal pains at\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "letters='qwertyuiopasdfghjklzxcvbnm'\n",
    "index=randint(0,25)\n",
    "\n",
    "print(complete_text(letters[index], temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
